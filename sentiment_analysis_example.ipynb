{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the project\n",
    "# Web scraping from https://proxyway.com/guides/web-scraping-with-python#:~:text=Steps%20to%20Build%20a%20Python%20Web%20Scraper%201,parameters.%20...%203%20Step%203%3A%20Write%20the%20Script\n",
    "url = \"https://www.irishgrassland.ie/journals/\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content)\n",
    "soup = soup.find('table')\n",
    "soup = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pdfs from Irish Grassland Journal and save them in the Raw folder\n",
    "# parse the url for the file name\n",
    "\n",
    "for element in soup:\n",
    "    url = element.get('href')\n",
    "    end = url.rfind('_')\n",
    "    if end > 0 : # set this condition as some of the links do not have the underscore in the file name. And these pdfs do not seem to be relevant to the project\n",
    "        start = end - 4\n",
    "        destFilename = 'Irish Grassland and Animal Production Association Journal' + url[start:end]\n",
    "        rq.urlretrieve(url, './Data/Raw/' + destFilename + '.pdf')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf677e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text from the pdfs and save them raw folder\n",
    "# reference https://www.geeksforgeeks.org/working-with-pdf-files-in-python/\n",
    "\n",
    "files = os.listdir('./Data/Raw/')\n",
    "files = filter(lambda f: f.endswith(('.pdf','.PDF')), files)\n",
    "\n",
    "for f in files:\n",
    "    pdfFileObj = open('./Data/Raw/' + f, 'rb')\n",
    "    pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "    num_pages = len(pdfReader.pages)\n",
    "    #pageobj = pdfReader.pages[num_pages + 1]\n",
    "    count = 0\n",
    "    text = \"\"\n",
    "    #text = pageobj.extractText()\n",
    "    txtFilename = './Data/Raw/' + f.replace('.pdf', '.txt')\n",
    "    \n",
    "    while count < num_pages:\n",
    "        pageObj = pdfReader.pages[count]\n",
    "        count +=1\n",
    "        text += pageObj.extract_text()\n",
    "   \n",
    "    txtFile = open(txtFilename, 'a', encoding=\"utf-8\")\n",
    "    txtFile.writelines(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d1db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the various articles that show in this search https://www.ifa.ie/?s=silage\n",
    "# There are 8 pages of results for this search, so I have to loop through the pages and extract the links pages are structured as https://www.ifa.ie/page/2/?s=silage\n",
    "\n",
    "# header code re engineered from https://stackoverflow.com/questions/41946166/requests-get-returns-403-while-the-same-url-works-in-browser\n",
    "# only want to declare once in the code\n",
    "h = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'} # This is chrome, you can set whatever browser you like\n",
    "\n",
    "# intialise search and load first page to get the number of pages\n",
    "soup = searchIFA(searchTerm = 'silage', pageNumber = 1, header = h)\n",
    "\n",
    "# find the number of pages returned by the search\n",
    "nav_results = soup.find_all(\"div\", {\"class\": \"nav-links\"})\n",
    "nav_results = nav_results[0].find_all('a')\n",
    "\n",
    "for element in nav_results:\n",
    "    if element.getText() == 'Next':\n",
    "        break\n",
    "    max_page = element.getText()\n",
    "    max_page = int(max_page)\n",
    "\n",
    "# loop through the pages and extract the links\n",
    "for page in range(1, max_page + 1):\n",
    "    print(page)\n",
    "    soup = searchIFA(searchTerm = 'silage', pageNumber = page, header = h)\n",
    "    searchLinks = load_links(soup = soup)\n",
    "    # now that I have the url, I can use beautiful soup to extract the text from the article\n",
    "    for item in searchLinks:\n",
    "        resultText(url = searchLinks[item], destFilename = item, header=h)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7677e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
